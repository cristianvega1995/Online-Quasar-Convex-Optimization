{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "af60d4a5-97b2-4f12-8499-e7744c0c48cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from scipy import stats\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d3e7d280-6dd4-4fbd-a79d-894468c3372f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Projection function to ensure the norm of x does not exceed the given radius\n",
    "def projection_func(x, radius):\n",
    "    norm_x = np.linalg.norm(x)\n",
    "    if norm_x == 0:\n",
    "        return x  # Return x unchanged if its norm is 0\n",
    "    if norm_x > radius:\n",
    "        x = (radius / norm_x) * x\n",
    "    return x\n",
    "    \n",
    "# Gradient of f(x)\n",
    "def grad_f(x, A, a):\n",
    "    # The gradient of f(x) = A x + a\n",
    "    return np.dot(A, x) + a\n",
    "\n",
    "# Function h(x) = f(x) / g(x)\n",
    "def h(x, A, a, alpha, b, beta):\n",
    "    return f(x, A, a, alpha) / g(x, b, beta)\n",
    "\n",
    "# Gradient of h(x)\n",
    "def grad_h(x, A, a, alpha, b, beta):\n",
    "    # Precompute g(x) and h(x) to avoid redundant calculations\n",
    "    g_x = g(x, b, beta)\n",
    "    h_x = h(x, A, a, alpha, b, beta)\n",
    "    \n",
    "    # Gradient of f divided by g(x)\n",
    "    grad_f_term = grad_f(x, A, a) / g_x\n",
    "    \n",
    "    # Gradient of the denominator g(x) = b\n",
    "    grad_g_term = (h_x / g_x) * b\n",
    "    \n",
    "    # Apply the quotient rule: (grad_f_term - grad_g_term) / g(x)\n",
    "    return grad_f_term - grad_g_term\n",
    "\n",
    "# Function f(x)\n",
    "def f(x, A, a, alpha):\n",
    "    return 0.5 * np.dot(x, np.dot(A, x)) + np.dot(a, x) + alpha\n",
    "\n",
    "# Function g(x)\n",
    "def g(x, b, beta):\n",
    "    return np.dot(b, x) + beta\n",
    "def create_positive_definite_matrix(n, m, a_1):\n",
    "    \"\"\"\n",
    "    Create an n x n positive definite matrix A with eigenvalues lower bounded by a_1.\n",
    "    \n",
    "    Parameters:\n",
    "    - n: Dimension of the matrix.\n",
    "    - a_1: Lower bound for the eigenvalues of A.\n",
    "    \n",
    "    Returns:\n",
    "    - A: Positive definite matrix with eigenvalues >= a_1.\n",
    "    \"\"\"\n",
    "    # Create a random n x n matrix\n",
    "    random_matrix = np.random.randn(m, n)\n",
    "    \n",
    "    # Symmetrize the matrix to ensure it's symmetric\n",
    "    symmetric_matrix = np.dot(random_matrix.T, random_matrix)\n",
    "    \n",
    "    # Add a_1 * I to ensure eigenvalues are bounded below by a_1\n",
    "    A = a_1*symmetric_matrix + a_1 * np.eye(n)\n",
    "    norm_A = np.linalg.norm(A, ord=2)\n",
    "    return A / norm_A\n",
    "\n",
    "# Function to compute the Lipschitz constant L\n",
    "def compute_L(norm_A, norm_a, alpha, norm_b, R, m):\n",
    "    # First term: ||A||^2 / m\n",
    "    term1 = (norm_A**2) / m\n",
    "    \n",
    "    # Second term: 2 * ||b|| * (||A|| * ||x|| + ||a||) / m^2\n",
    "    term2 = 2 * norm_b * (norm_A * R + norm_a) / (m**2)\n",
    "    \n",
    "    # Third term: (1/2 * ||A|| * ||x||^2 + ||a|| * ||x|| + alpha) * ||b||^2 / m^3\n",
    "    term3 = ((0.5 * norm_A * (R**2) + norm_a * R + alpha) * norm_b**2)    #/ (m**3)\n",
    "    \n",
    "    # Total L\n",
    "    L =  term1 + term2 + term3\n",
    "    \n",
    "    return L\n",
    "\n",
    "# Function to generate a vector b of norm R\n",
    "def gen_b(R, n):\n",
    "    # Generate a random n-dimensional vector\n",
    "    b_old = np.random.randn(n)\n",
    "    \n",
    "    # Compute the norm of b_old\n",
    "    norm_b_old = np.linalg.norm(b_old)\n",
    "    \n",
    "    # Avoid division by zero\n",
    "    if norm_b_old == 0:\n",
    "        return np.zeros_like(b_old)\n",
    "    \n",
    "    # Normalize b_old and scale by R\n",
    "    b = R * (b_old / norm_b_old)\n",
    "    return b\n",
    "\n",
    "def gen_b_new(b, t, R, n):\n",
    "    \"\"\"\n",
    "    Generate a new vector b, perturbed slightly with Gaussian noise.\n",
    "    \n",
    "    Parameters:\n",
    "    - b: Initial vector.\n",
    "    - t: Current iteration or time step (used for scaling the noise).\n",
    "    - R: Scaling factor for the final vector.\n",
    "    - n: Dimension of the vector.\n",
    "    \n",
    "    Returns:\n",
    "    - b: New perturbed and normalized vector scaled by R.\n",
    "    \"\"\"\n",
    "    # Generate a perturbed version of b\n",
    "    b_old = b + (0.1 / np.sqrt(t)) * np.random.randn(n)\n",
    "    \n",
    "    # Compute the norm of b_old\n",
    "    norm_b_old = np.linalg.norm(b_old)\n",
    "    \n",
    "    # Avoid division by zero by returning the zero vector if the norm is zero\n",
    "    if norm_b_old == 0:\n",
    "        return np.zeros_like(b_old)\n",
    "    \n",
    "    # Normalize b_old and scale by R\n",
    "    b_new = R * (b_old / norm_b_old)\n",
    "    return b_new\n",
    "\n",
    "def create_positive_definite_matrix_new(A, t, n, m, a_1):\n",
    "    \"\"\"\n",
    "    Create an n x n positive definite matrix with eigenvalues lower bounded by a_1.\n",
    "    \n",
    "    Parameters:\n",
    "    - A: Existing matrix to be perturbed.\n",
    "    - t: Current iteration or time step (used for scaling).\n",
    "    - n: Dimension of the matrix.\n",
    "    - m: The number of rows for generating a random matrix.\n",
    "    - a_1: Lower bound for the eigenvalues of A.\n",
    "    \n",
    "    Returns:\n",
    "    - A_new: A new positive definite matrix with eigenvalues >= a_1.\n",
    "    \"\"\"\n",
    "    # Generate a random m x n matrix\n",
    "    random_matrix = np.random.randn(m, n)\n",
    "    \n",
    "    # Symmetrize the matrix to ensure it's symmetric and positive semi-definite\n",
    "    symmetric_matrix = np.dot(random_matrix.T, random_matrix)\n",
    "    \n",
    "    # Add a_1 * I to ensure eigenvalues are bounded below by a_1\n",
    "    A_new = A + (0.1 / np.sqrt(t)) * (a_1 * symmetric_matrix + a_1 * np.eye(n))\n",
    "    \n",
    "    # Normalize A to keep the norm bounded\n",
    "    norm_A_new = np.linalg.norm(A_new, ord=2)\n",
    "    \n",
    "    return A_new / norm_A_new\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "211aae9d-fe4b-4650-b590-622bf7144cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "def subroutine(grad_h, projection_func, eta, max_iterations, initial_point, radius, tol,A,a,b,alpha,beta, norm_A,norm_a,norm_b):\n",
    "   \n",
    "    # Initialize the point and regret tracking\n",
    "    x = initial_point\n",
    "    x_old = x\n",
    "    error = 1e10\n",
    "    iter = 0\n",
    "    # while error>tol or iter<max_iterations:\n",
    "    while  error>tol :\n",
    "        iter = iter + 1\n",
    "        # Compute the gradient at the current point\n",
    "        grad = grad_h(x, A, a, alpha, b, beta)\n",
    "        x_old = x\n",
    "        # Gradient descent update step\n",
    "        x = x - eta * grad\n",
    "\n",
    "        # Projection step to ensure the updated point is within the feasible set\n",
    "        x = projection_func(x, radius)\n",
    "        error = np.linalg.norm(x-x_old) /np.linalg.norm(x)  \n",
    "        # print(iter)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fe6b6f5f-d265-44ae-bd76-264485a685c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "0.1\n",
      "0.1\n",
      "1.0000000000000004\n",
      "0.1\n",
      "71.22000000000001\n",
      "0.0014040999719180003\n"
     ]
    }
   ],
   "source": [
    "# Parameters\n",
    "max_iterations = 100000\n",
    "radius = 100\n",
    "dim = 50\n",
    "m = 1\n",
    "# Calculate norms\n",
    "norm_A = 1  # ||A||\n",
    "norm_b = 0.1  # ||b||\n",
    "norm_a = 0.1  # ||a||\n",
    "A = create_positive_definite_matrix(dim, int(0.1*dim), 1)\n",
    "b = gen_b(norm_b, dim)\n",
    "a = gen_b(norm_a, dim )\n",
    "beta = m+norm_b*radius\n",
    "alpha = 10\n",
    "tol = 1e-6\n",
    "M = norm_b*radius\n",
    "kappa = m / M\n",
    "initial_point = np.zeros(dim)\n",
    "L = compute_L(norm_A, norm_a, alpha, norm_b, radius, m)\n",
    "eta = (kappa / L)\n",
    "print(norm_A)\n",
    "print(norm_a)\n",
    "print(norm_b)\n",
    "print(np.linalg.norm(A, ord=2))\n",
    "print(kappa)\n",
    "print(L)\n",
    "print(eta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f6d221a-021c-40ca-a25e-5a8064a90587",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_point = subroutine(grad_h, projection_func, eta, max_iterations, initial_point, radius, tol,A,a,b,alpha,beta,norm_A,norm_a,norm_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6059e01f-a474-4640-a2c7-d17925d4c980",
   "metadata": {},
   "outputs": [],
   "source": [
    "A = create_positive_definite_matrix_new(A,2,dim, dim, 1)\n",
    "b = gen_b_new(b,2,0.1, dim)\n",
    "a = gen_b_new(a,2,0.1, dim )\n",
    "initial_point = subroutine(grad_h, projection_func, eta, max_iterations, initial_point, radius, tol,A,a,b,alpha,beta,norm_A,norm_a,norm_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d4c2140-bea3-4161-b2e7-a32c212e65a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "A = create_positive_definite_matrix_new(A,3,dim, int(0.1*dim), 1)\n",
    "b = gen_b_new(b,3,0.1, dim)\n",
    "a = gen_b_new(a,3,0.1, dim )\n",
    "initial_point = subroutine(grad_h, projection_func, eta, max_iterations, initial_point, radius, tol,A,a,b,alpha,beta,norm_A,norm_a,norm_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6237b4c-b2d3-4ab2-adea-743bcf67109c",
   "metadata": {},
   "outputs": [],
   "source": [
    "A = create_positive_definite_matrix_new(A,4,dim, int(0.1*dim), 1)\n",
    "b = gen_b_new(b,4,0.1, dim)\n",
    "a = gen_b_new(a,4,0.1, dim )\n",
    "initial_point = subroutine(grad_h, projection_func, eta, max_iterations, initial_point, radius, tol,A,a,b,alpha,beta,norm_A,norm_a,norm_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29b86d5a-c511-4f75-a507-6c172e555f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "A = create_positive_definite_matrix_new(A,5,dim, int(0.1*dim), 1)\n",
    "b = gen_b_new(b,5,0.1, dim)\n",
    "a = gen_b_new(a,5,0.1, dim )\n",
    "initial_point = subroutine(grad_h, projection_func, eta, max_iterations, initial_point, radius, tol,A,a,b,alpha,beta,norm_A,norm_a,norm_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52e33de6-ce41-48d1-9add-007cbf21d751",
   "metadata": {},
   "outputs": [],
   "source": [
    "def online_projected_gradient_descent(grad_h, projection_func, num_iterations, initial_point, radius, tol,dim,M,m, norm_b, norm_a, norm_A):\n",
    "    \"\"\"\n",
    "    Online Projected Gradient Descent (OPGD) Algorithm.\n",
    "\n",
    "    Parameters:\n",
    "    - grad_h: function to compute the gradient of the objective function at the current point.\n",
    "    - projection_func: function to project the point back to the feasible set.\n",
    "    - eta: the learning rate or step size for the gradient descent update.\n",
    "    - num_iterations: number of iterations to run the algorithm.\n",
    "    - initial_point: starting point for the algorithm.\n",
    "    - radius: radius of the feasible set.\n",
    "\n",
    "    Returns:\n",
    "    - regret_values: array of cumulative regret values at each iteration.\n",
    "    \"\"\"\n",
    "    # Initialize the point and regret tracking\n",
    "    x = initial_point\n",
    "    sol_t = initial_point\n",
    "    cumulative_regret = 0\n",
    "    regret_values = np.zeros(num_iterations)\n",
    "    m = 1\n",
    "    beta = m+norm_b*radius\n",
    "    alpha = 10\n",
    "    M = norm_b*radius\n",
    "    kappa = m / M\n",
    "    L = compute_L(norm_A, norm_a, alpha, norm_b, radius, m)\n",
    "    eta = (kappa / L)\n",
    "    # Randomly generate the parameters for the function h\n",
    "    A = create_positive_definite_matrix(dim, dim, 1)\n",
    "    b = gen_b(norm_b, dim)\n",
    "    a = gen_b(norm_a, dim )\n",
    "    sol_t = subroutine(grad_h, projection_func, eta, max_iterations, sol_t, radius,tol,A,a,b,alpha,beta,norm_A,norm_a,norm_b)\n",
    "    for t in range(num_iterations):\n",
    "        # Update the cumulative regret and store its value\n",
    "        # cumulative_regret += max((h(x, A, a, alpha, b, beta)-h(sol_t, A, a, alpha, b, beta)),0)\n",
    "        A = create_positive_definite_matrix_new(A,t+1,dim, dim, 1)\n",
    "        b = gen_b_new(b,t+1,norm_b, dim)\n",
    "        a = gen_b_new(a,t+1,norm_a, dim )\n",
    "        sol_t = subroutine(grad_h, projection_func, eta, max_iterations, sol_t, radius, tol,A,a,b,alpha,beta,norm_A,norm_a,norm_b)\n",
    "        cumulative_regret += (h(x, A, a, alpha, b, beta)-h(sol_t, A, a, alpha, b, beta))\n",
    "        # Compute the gradient at the current point\n",
    "        grad = grad_h(x, A, a, alpha, b, beta)\n",
    "\n",
    "        # Gradient descent update step\n",
    "        x = x - eta * grad\n",
    "        # print((h(x, A, a, alpha, b, beta)-h(sol_t, A, a, alpha, b, beta))) print(t)\n",
    "        # Projection step to ensure the updated point is within the feasible set\n",
    "        x = projection_func(x, radius)\n",
    "\n",
    "        # Update the cumulative regret and store its value\n",
    "        #cumulative_regret += h(x, a, b)\n",
    "        regret_values[t] = cumulative_regret\n",
    "    return regret_values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92fb2a6f-dd1b-43f5-9c16-6275800a2a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "num_iterations = 20000\n",
    "radius = 10\n",
    "dim = 50\n",
    "m = 0.1\n",
    "# Calculate norms\n",
    "norm_A = 1  # ||A||\n",
    "norm_b = 0.1  # ||b||\n",
    "norm_a = 0.1  # ||a||\n",
    "beta = m+norm_b*radius\n",
    "alpha = 10\n",
    "tol = 1e-6\n",
    "M = norm_b*radius\n",
    "kappa = m / M\n",
    "initial_point = np.zeros(dim)\n",
    "L = compute_L(norm_A, norm_a, alpha, norm_b, radius, m)\n",
    "eta = (kappa / L)\n",
    "regret_values = online_projected_gradient_descent(grad_h, projection_func, num_iterations, initial_point, radius, tol,dim,M,m, norm_b, norm_a, norm_A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17a63b52-1af6-449a-b888-d64f3bba8b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_regret(regret_values, num_iterations):\n",
    "    iterations = np.arange(1, num_iterations + 1)\n",
    "\n",
    "    # Plot of regret values over iterations\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    plt.plot(iterations, regret_values, label='Regret', color='blue')\n",
    "    plt.title('Regret over Iterations')\n",
    "    plt.xlabel('Iteration')\n",
    "    plt.ylabel('Regret')\n",
    "    plt.grid(True)\n",
    "\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "# Plot the results\n",
    "plot_regret(regret_values, num_iterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "621d20b7-b101-452d-920e-07d1372c078c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_regrets(cases, grad_h, projection_func, num_iterations, initial_point, radius, tol,dim,M,m, norm_b, norm_a, norm_A):\n",
    "    iterations = np.arange(1, num_iterations + 1)\n",
    "    cases = int(cases)\n",
    "    \n",
    "    # To store regret values for multiple cases\n",
    "    regret_values = np.zeros((num_iterations, cases))\n",
    "    \n",
    "    # Running the algorithm for each case\n",
    "    for i in range(cases):\n",
    "        print(i)\n",
    "        initial_point = np.zeros(dim)\n",
    "        regret_values[:, i] = online_projected_gradient_descent(grad_h, projection_func, num_iterations, initial_point, radius, tol,dim,M,m, norm_b, norm_a, norm_A)\n",
    "    \n",
    "    # Compute mean and standard deviation for plotting shaded regions\n",
    "    mean_regret = np.mean(regret_values, axis=1)\n",
    "    std_regret = np.std(regret_values, axis=1)\n",
    "\n",
    "    # Plot of regret values over iterations\n",
    "    plt.figure(figsize=(10, 6))\n",
    "\n",
    "    # Plot the mean regret line\n",
    "    plt.plot(iterations, mean_regret, label='Mean Regret', color='green', lw=2)\n",
    "    plt.legend(loc='best',fontsize='small')\n",
    "    # Plot shaded area (mean ± std_dev)\n",
    "    plt.fill_between(iterations, mean_regret - std_regret, mean_regret + std_regret, \n",
    "                     color='green', alpha=0.2, label='Regret ± Std Dev')\n",
    "\n",
    "    # Add labels and grid\n",
    "    plt.title('Regret over Iterations')\n",
    "    plt.xlabel('Iteration')\n",
    "    plt.ylabel('Averaged regret')\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('ogd.jpeg')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88939243-4b4f-48c5-97a1-1e67b4a34778",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "num_iterations = 8000\n",
    "radius = 10\n",
    "dim = 50\n",
    "m = 1\n",
    "# Calculate norms\n",
    "norm_A = 1  # ||A||\n",
    "norm_b = 0.1  # ||b||\n",
    "norm_a = 0.1  # ||a||\n",
    "beta = m+norm_b*radius\n",
    "alpha = 10\n",
    "tol = 1e-6\n",
    "M = norm_b*radius\n",
    "kappa = m / M\n",
    "initial_point = np.zeros(dim)\n",
    "L = compute_L(norm_A, norm_a, alpha, norm_b, radius, m)\n",
    "eta = (kappa / L)\n",
    "cases = 20\n",
    "print(norm_A)\n",
    "print(norm_a)\n",
    "print(norm_b)\n",
    "print(np.linalg.norm(A, ord=2))\n",
    "print(kappa)\n",
    "print(L)\n",
    "print(eta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68e6922e-712c-4d17-9cd4-f0d16d2089a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_regrets(cases, grad_h, projection_func, num_iterations, initial_point, radius, tol,dim,M,m, norm_b, norm_a, norm_A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a47cc5d-0bfc-4451-bfbc-66c50626f5c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(norm_A)\n",
    "print(norm_a)\n",
    "print(norm_b)\n",
    "print(np.linalg.norm(A, ord=2))\n",
    "print(kappa)\n",
    "print(L)\n",
    "print(eta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cf04673-afe3-4180-a060-954df420b83d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def online_delayed_projected_gradient_descent(grad_h, projection_func, eta, num_iterations, initial_point, radius, vec_delays,max_delay,tol,dim,M,m, norm_b, norm_a, norm_A):\n",
    "    \"\"\"\n",
    "    Online Delayed Projected Gradient Descent Algorithm.\n",
    "\n",
    "    Parameters:\n",
    "    - grad_h: function to compute the gradient of the objective function.\n",
    "    - projection_func: function to project the point back to the feasible set.\n",
    "    - eta: the learning rate or step size for the gradient descent update.\n",
    "    - num_iterations: number of iterations to run the algorithm.\n",
    "    - initial_point: starting point for the algorithm.\n",
    "    - radius: radius of the feasible set.\n",
    "    - vec_delays: list of delays indicating when gradients arrive.\n",
    "\n",
    "    Returns:\n",
    "    - regret_values: array of cumulative regret values at each iteration.\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize the point, regret tracking, and storage for x values\n",
    "    x = initial_point\n",
    "    cumulative_regret = 0\n",
    "    regret_values = np.zeros(num_iterations)\n",
    "    vec_iter = np.zeros((len(initial_point), num_iterations + int(max_delay)))\n",
    "    vec_sol = np.zeros((len(initial_point), num_iterations + int(max_delay)))\n",
    "    vec_b = np.zeros((len(initial_point), num_iterations + int(max_delay)))\n",
    "    vec_a = np.zeros((len(initial_point), num_iterations + int(max_delay)))\n",
    "    matrices = np.zeros((len(initial_point), len(initial_point), num_iterations + int(max_delay)))\n",
    "    m = 1\n",
    "    beta = m+norm_b*radius\n",
    "    alpha = 10\n",
    "    M = norm_b*radius\n",
    "    kappa = m / M\n",
    "    L = compute_L(norm_A, norm_a, alpha, norm_b, radius, m)\n",
    "    eta = (kappa / L)\n",
    "    max_iterations = 1e8\n",
    "    # Randomly generate the parameters for the function h\n",
    "    A = create_positive_definite_matrix(dim, dim, 1)\n",
    "    b = gen_b(norm_b, dim)\n",
    "    a = gen_b(norm_a, dim )\n",
    "    sol_t = subroutine(grad_h, projection_func, eta, max_iterations, x, radius,tol,A,a,b,alpha,beta,norm_A,norm_a,norm_b)\n",
    "    for t in range(num_iterations+ int(max_delay)):\n",
    "        grad = np.zeros_like(x)  # Initialize gradient to zero for each iteration\n",
    "        vec_iter[:, t] = x  # Store the current point x in vec_iter\n",
    "\n",
    "        # Process delayed gradients that arrive at this time step\n",
    "        for j in vec_delays[t]:\n",
    "            # Update the cumulative regret and store its value\n",
    "            # cumulative_regret += max((h(x, A, a, alpha, b, beta)-h(sol_t, A, a, alpha, b, beta)),0)\n",
    "            A = create_positive_definite_matrix_new(A,j+1,dim, dim, 1)\n",
    "            b = gen_b_new(b,j+1,norm_b, dim)\n",
    "            a = gen_b_new(a,j+1,norm_a, dim )\n",
    "            sol_t = subroutine(grad_h, projection_func, eta, max_iterations, sol_t, radius, tol,A,a,b,alpha,beta,norm_A,norm_a,norm_b)\n",
    " \n",
    "            # Compute the delayed gradient at the stored point\n",
    "            x_old = vec_iter[:, j]\n",
    "            grad += grad_h(x_old, A, a, alpha, b, beta)\n",
    "            \n",
    "            # Update the regret value based on the delayed information\n",
    "            regret_values[j] =  (h(x_old, A, a, alpha, b, beta)-h(sol_t, A, a, alpha, b, beta))\n",
    "\n",
    "        # Gradient descent update step using the accumulated gradient\n",
    "        x = x - eta * grad\n",
    "\n",
    "        # Projection step to ensure the updated point is within the feasible set\n",
    "        x = projection_func(x, radius)\n",
    "    for k in range(num_iterations):\n",
    "        # Update the cumulative regret and store its value\n",
    "        cumulative_regret += regret_values[k]\n",
    "        regret_values[k] = cumulative_regret\n",
    "\n",
    "    return regret_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f15ece3-60ea-4aaf-b855-98625844d166",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_regrets_delayed(cases, grad_h, projection_func, eta, num_iterations, radius, dim, max_delay, tol,M,m, norm_b, norm_a, norm_A):\n",
    "    iterations = np.arange(1, num_iterations + 1)\n",
    "    cases = int(cases)\n",
    "    \n",
    "    # To store regret values for multiple cases\n",
    "    regret_values = np.zeros((num_iterations, cases))\n",
    "    \n",
    "    # Running the algorithm for each case\n",
    "    for i in range(cases):\n",
    "        print(i)\n",
    "        delays = np.random.randint(1, max_delay+1, size=(num_iterations, 1))\n",
    "        # Initialize vec_delays as a list of empty lists\n",
    "        vec_delays = [[] for _ in range(num_iterations + max_delay)]\n",
    "        # Populate vec_delays with the delayed indices\n",
    "        for j in range(num_iterations):\n",
    "            vec_delays[j + int(delays[i]) - 1].append(j)\n",
    "        initial_point = np.zeros(dim)        # Normalize the vector to have unit norm\n",
    "        regret_values[:, i] =online_delayed_projected_gradient_descent(grad_h, projection_func, eta, num_iterations, initial_point, radius, vec_delays,max_delay,tol,dim,M,m, norm_b, norm_a, norm_A)\n",
    "    \n",
    "    # Compute mean and standard deviation for plotting shaded regions\n",
    "    mean_regret = np.mean(regret_values, axis=1)\n",
    "    std_regret = np.std(regret_values, axis=1)\n",
    "\n",
    "    # Plot of regret values over iterations\n",
    "    plt.figure(figsize=(10, 6))\n",
    "\n",
    "    # Plot the mean regret line\n",
    "    plt.plot(iterations, mean_regret, label='Mean Regret', color='blue', lw=2)\n",
    "    plt.legend(loc='best',fontsize='small')\n",
    "\n",
    "    # Plot shaded area (mean ± std_dev)\n",
    "    plt.fill_between(iterations, mean_regret - std_regret, mean_regret + std_regret, \n",
    "                     color='blue', alpha=0.2, label='Regret ± Std Dev')\n",
    "\n",
    "    # Add labels and grid\n",
    "    plt.title('Regret over Iterations')\n",
    "    plt.xlabel('Iteration')\n",
    "    plt.ylabel('Averaged regret')\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('delayed.jpeg')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b9213f8-3346-49ed-b1d8-fa6da7ad2020",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "num_iterations = 8000\n",
    "radius = 10\n",
    "dim = 50\n",
    "m = 1\n",
    "# Calculate norms\n",
    "norm_A = 1  # ||A||\n",
    "norm_b = 0.1  # ||b||\n",
    "norm_a = 0.1  # ||a||\n",
    "beta = m+norm_b*radius\n",
    "alpha = 10\n",
    "tol = 1e-6\n",
    "M = norm_b*radius\n",
    "kappa = m / M\n",
    "delay = 10\n",
    "initial_point = np.zeros(dim)\n",
    "L = compute_L(norm_A, norm_a, alpha, norm_b, radius, m)\n",
    "eta = (kappa / L)/ (delay+np.sqrt(delay)*(delay-1))\n",
    "cases = 20\n",
    "plot_regrets_delayed(cases, grad_h, projection_func, eta, num_iterations, radius, dim, delay, tol,M,m, norm_b, norm_a, norm_A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b656bf-84bf-4c69-b3e2-efaba4d74696",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_regrets_delayeds(cases, grad_h, projection_func, eta, num_iterations, radius, dim, tol,M,m, norm_b, norm_a, norm_A):\n",
    "    iterations = np.arange(1, num_iterations + 1)\n",
    "    cases = int(cases)\n",
    "    delays_vec=[5,10,20]\n",
    "    colors=['aqua', 'maroon','fuchsia','orange','green','yellow']\n",
    "    count=-1\n",
    "    for t in delays_vec:\n",
    "        print(t)\n",
    "        count=count+1\n",
    "        # To store regret values for multiple cases\n",
    "        regret_values = np.zeros((num_iterations, cases))\n",
    "        # Running the algorithm for each case\n",
    "        eta =(kappa / L)/ (t+np.sqrt(t)*(t-1))\n",
    "        # Running the algorithm for each case\n",
    "        for i in range(cases):\n",
    "            print(i)\n",
    "            delays = np.random.randint(1, t+1, size=(num_iterations, 1))\n",
    "            # Initialize vec_delays as a list of empty lists\n",
    "            vec_delays = [[] for _ in range(num_iterations + t)]\n",
    "            # Populate vec_delays with the delayed indices\n",
    "            for j in range(num_iterations):\n",
    "                vec_delays[j + int(delays[i]) - 1].append(j)\n",
    "            initial_point = np.zeros(dim)        # Normalize the vector to have unit norm\n",
    "            regret_values[:, i] =online_delayed_projected_gradient_descent(grad_h, projection_func, eta, num_iterations, initial_point, radius, vec_delays,t,tol,dim,M,m, norm_b, norm_a, norm_A)\n",
    "    \n",
    "        # Compute mean and standard deviation for plotting shaded regions\n",
    "        mean_regret = np.mean(regret_values, axis=1)\n",
    "        std_regret = np.std(regret_values, axis=1)\n",
    "\n",
    "        # Plot of regret values over iterations\n",
    "        # plt.figure(figsize=(10, 6))\n",
    "\n",
    "        # Plot the mean regret line\n",
    "        plt.plot(iterations, mean_regret, label=f\"Delay {t}\", color=colors[count], lw=2)\n",
    "        plt.legend(loc='best',fontsize='small')\n",
    "        # Plot shaded area (mean ± std_dev)\n",
    "        # plt.fill_between(iterations, mean_regret - std_regret, mean_regret + std_regret, \n",
    "        #              color=colors[count], alpha=0.2, label=f\"Regret ± Std Dev with maximun delay {t}\")\n",
    "        plt.fill_between(iterations, mean_regret - std_regret, mean_regret + std_regret, \n",
    "                     color=colors[count], alpha=0.2)\n",
    "\n",
    "    # Add labels and grid\n",
    "    plt.title('Regret over Iterations')\n",
    "    plt.xlabel('Iteration')\n",
    "    plt.ylabel('Averaged regret')\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('Delayed_cases1.jpeg')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd3ad348-c7e4-4f71-be7a-9d1d7a897b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "num_iterations = 10000\n",
    "radius = 10\n",
    "dim = 50\n",
    "m = 1\n",
    "# Calculate norms\n",
    "norm_A = 1  # ||A||\n",
    "norm_b = 0.1  # ||b||\n",
    "norm_a = 0.1  # ||a||\n",
    "beta = m+norm_b*radius\n",
    "alpha = 10\n",
    "tol = 1e-10\n",
    "M = norm_b*radius\n",
    "kappa = m / M\n",
    "initial_point = np.zeros(dim)\n",
    "L = compute_L(norm_A, norm_a, alpha, norm_b, radius, m)\n",
    "eta = (kappa / L)/ (delay+np.sqrt(delay)*(delay-1))\n",
    "cases = 20\n",
    "print(norm_A)\n",
    "print(norm_a)\n",
    "print(norm_b)\n",
    "print(np.linalg.norm(A, ord=2))\n",
    "print(kappa)\n",
    "print(L)\n",
    "print(eta)\n",
    "plot_regrets_delayeds(cases, grad_h, projection_func, eta, num_iterations, radius, dim, tol,M,m, norm_b, norm_a, norm_A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b88045e7-bc3f-42c8-b482-c69932209445",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4683a8c5-ec14-45f2-8da6-086bca045e60",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd389250-763c-4883-8ff8-90028ec9d4e7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
